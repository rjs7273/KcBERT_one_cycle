# T4 GPU ëŸ°íƒ€ì„

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from transformers import AutoTokenizer, AutoModel, AdamW
from tqdm import tqdm  # tqdm ì„í¬íŠ¸
import numpy as np
import os
import re

# ------------------------
# 1. ë°ì´í„°ì…‹ ì •ì˜
# ------------------------
class SentimentDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in encoding.items()}
        if self.labels is not None:
            item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

# ------------------------
# 2. ëª¨ë¸ ì •ì˜
# ------------------------
class KcBERTSentiment(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = AutoModel.from_pretrained("beomi/KcELECTRA-base")
        self.dropout = nn.Dropout(0.1)
        self.linear = nn.Linear(self.bert.config.hidden_size, 3)  # [fear, neutral, greed]
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        pooled = outputs[:, 0]  # CLS í† í°
        logits = self.linear(self.dropout(pooled))
        probs = self.softmax(logits)
        return probs

# ------------------------
# 3. Threshold ìŠ¤ì¼€ì¤„
# ------------------------
def get_threshold(epoch):
    if epoch < 2:
        return 1.0
    elif epoch < 4:
        return 0.9
    elif epoch < 6:
        return 0.7
    elif epoch < 8:
        return 0.6
    else:
        return 0.5

# ------------------------
# 4. í•™ìŠµ ë£¨í”„
# ------------------------
def train(model, dataloader, optimizer, device):
    model.train()
    loss_fn = nn.MSELoss()
    total_loss = 0
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  ë°” ì¶”ê°€
    for batch in tqdm(dataloader, desc="Training", ncols=100, leave=False):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# ------------------------
# 5. ê°€ì¥ ìµœê·¼ ì—í¬í¬ ë¡œë“œ
# ------------------------

model_dir = "/content/drive/MyDrive/KcBERT/model"
pattern = re.compile(r"kcbert_epoch(\d+)\.pt")

def find_latest_model(model_dir):
    latest_epoch = -1
    latest_path = None
    for fname in os.listdir(model_dir):
        match = pattern.match(fname)
        if match:
            epoch = int(match.group(1))
            if epoch > latest_epoch:
                latest_epoch = epoch
                latest_path = os.path.join(model_dir, fname)
    return latest_path, latest_epoch

# ------------------------
# 6. ì „ì²´ í”„ë¡œì„¸ìŠ¤
# ------------------------
def main():
    # ë°ì´í„° ë¡œë“œ
    labeled_df = pd.read_csv("/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_sample_with_label_1_to_10.csv")
    full_df = pd.read_csv("/content/drive/MyDrive/KcBERT/data/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_preprocess.csv")

    labeled_df = labeled_df.dropna(subset=["content"])
    full_df = full_df.dropna(subset=["content"])

    tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ëª¨ë¸ ìƒì„± ë° ë¡œë”©
    model = KcBERTSentiment()
    latest_model_path, latest_epoch = find_latest_model(model_dir)

    if latest_model_path:
        model.load_state_dict(torch.load(latest_model_path, map_location="cpu"))
        model.eval()
        print(f"âœ… ê°€ì¥ ìµœê·¼ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {latest_model_path} (epoch {latest_epoch})")
    else:
        print("ğŸš€ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

    model = model.to(device)

    # ì´ˆê¸° ë¼ë²¨ ë°ì´í„°ì…‹
    labeled_texts = labeled_df["content"].tolist()
    labeled_labels = labeled_df[["fear_score", "neutral_score", "greed_score"]].values.tolist()
    labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)
    labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=2e-5)

    # ì—í¬í¬ ë²”ìœ„ ìˆ˜ì •: ìµœì‹  ì—í¬í¬ë¶€í„° í•™ìŠµ ì‹œì‘
    for epoch in range(latest_epoch, 10):  # ìµœì‹  ì—í¬í¬ë¶€í„° ì‹œì‘
        print(f"\nğŸŒ€ Epoch {epoch+1}")
        loss = train(model, labeled_loader, optimizer, device)
        print(f"Train loss: {loss:.4f}")

        # ëª¨ë¸ ì €ì¥
        save_path = f"/content/drive/MyDrive/KcBERT/model/kcbert_epoch{epoch+1}.pt"
        torch.save(model.state_dict(), save_path)
        print(f"ğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")

        # ---------- Pseudo-labeling ----------
        threshold = get_threshold(epoch)
        print(f"Using threshold: {threshold}")
        model.eval()

        full_texts = full_df["content"].tolist()
        full_dataset = SentimentDataset(full_texts, tokenizer=tokenizer)
        full_loader = DataLoader(full_dataset, batch_size=64)

        pseudo_texts, pseudo_labels = [], []

        with torch.no_grad():
            for batch in full_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                outputs = model(input_ids, attention_mask)  # (batch_size, 3)
                confidences, _ = torch.max(outputs, dim=1)
                mask = confidences > threshold
                for i in range(len(mask)):
                    if mask[i]:
                        pseudo_texts.append(tokenizer.decode(batch["input_ids"][i], skip_special_tokens=True))
                        pseudo_labels.append(outputs[i].cpu().numpy())

        # ìƒˆë¡­ê²Œ í™•ë³´ëœ pseudo-labeled ë°ì´í„°ì…‹ ìƒì„±
        if pseudo_labels:
            print(f"  âœ… Accepted pseudo-labels: {len(pseudo_labels)}")
            labeled_texts += pseudo_texts
            labeled_labels += pseudo_labels
            labeled_dataset = SentimentDataset(labeled_texts, labeled_labels, tokenizer)
            labeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True)
        else:
            print("  âš ï¸ No pseudo-labels accepted this round.")

if __name__ == "__main__":
    main()
